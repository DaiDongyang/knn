# KNN实验
#### 代东洋, 2017210885, ddy17@mails.tsinghua.edu.cn
## 简介
本次实验使用K-近邻算法对手写数字进行分类。

涉及的算法包括：

+ 寻找k近邻：
	+ 使用欧式距离寻找k近邻
	+ 使用马氏距离寻找k近邻
	+ 在欧式距离的情况下，使用KD树改进节点检索，寻找k近邻
+ 根据k近邻确定测试样本的类别：
	+ 基本的KNN算法
	+ 改进权值的wknn算法
+ 使用pca算法进行降维

代码使用Python3语言，代码依赖numpy、tqdm库。其中tqdm提供进度条的功能，便于观察实验的进度。

## 算法原理
#### 寻找k近邻
顾名思义，寻找k近邻就是寻找距离测试样本最近的k个最近的训练样本（以后会根据这k个训练样本确定该测试样本的类别）。本实验中涉及到的实验的度量方法有两种：欧氏距离和马氏距离。
##### 欧氏距离
样本 $x=(x_1, x_2, ...x_d)^T$ 和样本 $y = (y_1, y_2, ... y_d)^T$ 之间的欧氏距离为：
$$d = \sqrt{(x-y)^T(x-y)}$$
##### 马氏距离
样本 $x=(x_1, x_2, ...x_d)^T$ 和样本 $y = (y_1, y_2, ... y_d)^T$ 之间的马氏距离为：
$$d = \sqrt{(x-y)^T\Sigma^{-1}(x-y)}$$
其中$\Sigma$为样本协方差矩阵，根据ppt内容，应该是训练集和测试样本组成的集合的协方差矩阵，但由于训练集有1000多个样本，单个测试样本对协方差矩阵的影响很小，所以只算了训练集的协方差矩阵。这样的一个好处是，可以预先把协方差矩阵以及它的逆算好，不必每测试时都算一遍协方差矩阵，节省了程序的运行时间。

##### 欧式距离+KD树
kd树（ k-维树的缩写，）是在k维欧几里德空间组织点的数据结构。kd树的k和knn的k不一样，下面出现的k如无特别说明一般指的是knn的k。关于kd树的详细介绍参见李航的《统计学习基础》第41~44页，不再累述，这里主要介绍一下自己实现时的一些做法。

本实验中选择轮流的方法选择垂直分区面（有些地方先选择方差大的做垂直分区面，不过本实验中我实现的比较简单，就是依次按照1、2...d维特征作为分区考虑的特征）。

kd树每一个子树对应于一个超矩形区域。如果测试样本到超矩形区域的最小距离都比搜索过程中当前k近邻中最大的距离要大，那没必要对该超矩形区域进行搜索。因此，为了方便计算测试样本到每个子树的超区域的最小距离，kd树的每个节点用两个向量分别表示了该超区域每个维度下的最小值和最大值。

李航那本书上提供的利用kd树求最近邻的算法，而本实验中需要求k近邻。因此，本实验中用到了一个固定容量为k的最大堆，用来存储搜索过程中k个距离最小的训练样本。最大堆有个函数`get_max_dist_sq()` 返回当前最大堆的最大值，如果最大堆达到容量k时，该函数返回`float('inf')`,然后最大堆使用`update_value()`函数更新最大堆，该函数会把新加入的数据替换掉最大堆中的最大数据。这样，在kd树中进行搜索时，只要判断当前比较的训练样本的距离（的平方）是否小于`get_max_dist_sq()`即可。如果小于`get_max_dist_sq()`，则使用`update_value()`函数把当前训练样本放入最小堆即可。当程序从叶节点搜索到根节点结束后，最大堆中的k个值即为k个近邻。

##### 马氏距离+KD树
马氏距离+KD树的情况没能实现，可以作为以后进一步改进的一个方向。

由于kd树欧几里德空间组织点的数据结构，欧氏距离最小时不一定对应的马氏距离也最小，所以无法直接计算测试样本到超区域的最小距离。

但是根据欧氏距离的公式

$$d = \sqrt{(x-y)^T\Sigma^{-1}(x-y)}$$

其中$x=(x_1, x_2, ...x_d)^T$ ， $y = (y_1, y_2, ... y_d)^T$ 

不考虑$\Sigma$不可逆的情况，由于$\Sigma$是实对称矩阵，因此
$$\Sigma = Q \Lambda Q^T$$

其中$Q$是正交矩阵，$\Lambda$是对角阵，且对角线每一个元素都是$\Sigma$的特征值。$\Lambda^{-1}$即为$\Lambda$对角线每一个元素取倒数。所以：
$$\Sigma^{-1} = Q \Lambda^{-1} Q^T$$

令$B=\Lambda^{-1}$， $P=Q^T$，显然$B$是对角阵，$P$是正交阵。

$$ \Sigma^{-1} = P^T B P$$
$$ \Sigma^{-1} = (P B^{1/2})^T(B^{1/2}P)$$

令$M = B^{1/2}P$，因此
$$\Sigma^{-1} = M^TM$$
$$d = \sqrt{(x-y)^T\Sigma^{-1}(x-y)} $$
$$d = \sqrt{(x-y)^TM^TM(x-y)}$$
$$d = \sqrt{(Mx - My)^T(Mx-My)}$$

令$x^{'} = Mx$，$y^{'} = My$，则
$$d = \sqrt{(x^{'} - y^{'})^T(x^{'}-y^{'})}$$

说明马氏距离就是特征进行线性变换后的欧氏距离，其中变换矩阵为$M$。

这样，就可以把马氏距离转化为欧氏距离。（不过，使用这个方法我测试样本的正确率不到20%，还没找到bug在哪，来不及实现这个方法了。。。，对应的代码核心部分在knn.py的`get_ints_m_trans_matrix`函数中，已经被注释掉。。。）。

#### 确定测试样本的类别
#### 降维
## 重要代码

## 实验过程

## 结果分析